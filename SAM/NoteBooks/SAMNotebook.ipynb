{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Segmentation Pipeline using Segment Anything Model (SAM)\n",
    "This notebook provides an end-to-end pipeline for image segmentation using Meta AI's Segment Anything Model (SAM). It includes functions for loading the model, segmenting images, visualizing results, cropping and collaging segments, and monitoring system resources during processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msegment_anything\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sam_model_registry, SamAutomaticMaskGenerator\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\cv2\\__init__.py:181\u001b[0m\n\u001b[0;32m    176\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m DEBUG: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtra Python code for\u001b[39m\u001b[38;5;124m\"\u001b[39m, submodule, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis loaded\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m DEBUG: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOpenCV loader: DONE\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 181\u001b[0m \u001b[43mbootstrap\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\cv2\\__init__.py:153\u001b[0m, in \u001b[0;36mbootstrap\u001b[1;34m()\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DEBUG: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRelink everything from native cv2 module to cv2 package\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    151\u001b[0m py_module \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mmodules\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 153\u001b[0m native_module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcv2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    155\u001b[0m sys\u001b[38;5;241m.\u001b[39mmodules[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv2\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m py_module\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28msetattr\u001b[39m(py_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_native\u001b[39m\u001b[38;5;124m\"\u001b[39m, native_module)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2032.0_x64__qbz5n2kfra8p0\\Lib\\importlib\\__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Import Libraries \n",
    "import torch\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "import GPUtil\n",
    "import threading\n",
    "import time\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "## Resource Monitoring\n",
    "\n",
    "The next cell contains functions for monitoring system resources such as CPU, memory, and GPU usage. These functions will be used to track resource utilization during the image segmentation process.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resource Monitoring\n",
    "\n",
    "# Function to get resource usage\n",
    "def get_resource_usage():\n",
    "    # Get CPU usage\n",
    "    cpu_percent = psutil.cpu_percent()\n",
    "    # Get memory usage\n",
    "    memory = psutil.virtual_memory()\n",
    "    memory_percent = memory.percent\n",
    "    # Get GPU usage\n",
    "    gpus = GPUtil.getGPUs()\n",
    "    if gpus:\n",
    "        gpu = gpus[0]\n",
    "        gpu_percent = gpu.load * 100\n",
    "        gpu_memory_used = gpu.memoryUsed\n",
    "        gpu_memory_total = gpu.memoryTotal\n",
    "        gpu_memory_percent = (gpu_memory_used / gpu_memory_total) * 100\n",
    "    else:\n",
    "        gpu_percent = 0\n",
    "        gpu_memory_percent = 0\n",
    "    return f\"CPU:{cpu_percent:.1f}%, Mem:{memory_percent:.1f}%, GPU:{gpu_percent:.1f}%, GPU Mem:{gpu_memory_percent:.1f}%\"\n",
    "\n",
    "# Resource monitor function\n",
    "def resource_monitor(pbar, stop_event, pbar_lock):\n",
    "    while not stop_event.is_set():\n",
    "        resource_usage = get_resource_usage()\n",
    "        with pbar_lock:\n",
    "            pbar.set_postfix_str(resource_usage)\n",
    "        time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "## Initialize Segment Anything Model (SAM)\n",
    "\n",
    "In the next cell, we initialize the Segment Anything Model (SAM) using a pre-trained checkpoint. This model is used for automatic mask generation in image segmentation tasks. Below are the parameters used in the initialization function:\n",
    "\n",
    "- **sam_checkpoint**: Path to the pre-trained SAM model checkpoint file.\n",
    "- **model_type**: Type of the model architecture. In this case, it is \"vit_h\".\n",
    "- **device**: The device on which the model will run. It uses \"cuda\" if a GPU is available, otherwise it falls back to \"cpu\".\n",
    "- **points_per_side**: Number of points to sample per side of the image for mask generation.\n",
    "- **pred_iou_thresh**: Threshold for the predicted Intersection over Union (IoU) score. Masks with IoU scores below this threshold are discarded.\n",
    "- **stability_score_thresh**: Threshold for the stability score of the mask. Masks with stability scores below this threshold are discarded.\n",
    "- **crop_n_layers**: Number of layers to crop from the image.\n",
    "- **crop_n_points_downscale_factor**: Factor to downscale the number of points when cropping.\n",
    "- **min_mask_region_area**: Minimum area (in pixels) for a mask region to be considered valid.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Segment Anything Model\n",
    "def initialize_sam():\n",
    "    sam_checkpoint = \"c:\\\\Users\\\\Riley\\\\Desktop\\\\sam_vit_h_4b8939.pth\"  # Update this path as needed\n",
    "    model_type = \"vit_h\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "    sam.to(device=device)\n",
    "    return SamAutomaticMaskGenerator(\n",
    "        sam,\n",
    "        points_per_side=8,  # Number of points to sample per side of the image\n",
    "        pred_iou_thresh=0.90,  # Threshold for the predicted Intersection over Union (IoU) score\n",
    "        stability_score_thresh=0.95,  # Threshold for the stability score of the mask\n",
    "        crop_n_layers=0,  # Number of layers to crop from the image\n",
    "        crop_n_points_downscale_factor=2,  # Factor to downscale the number of points when cropping\n",
    "        min_mask_region_area=5500,  # Minimum area (in pixels) for a mask region to be considered valid\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "## Output Full Segmented Image\n",
    "\n",
    "The next cell contains a function to output the full segmented image. This function takes an image and its corresponding masks, combines all the masks into a single mask, applies this mask to the image, and saves the segmented image to the specified output path.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the full segmented image\n",
    "def output_full_segmented_image(image, masks, output_base_path):\n",
    "    # Create an empty mask with the same dimensions as the image\n",
    "    full_mask = np.zeros(image.shape[:2], dtype=np.uint8)\n",
    "\n",
    "    # Combine all masks into the full mask\n",
    "    for mask in masks:\n",
    "        full_mask[mask['segmentation']] = 255\n",
    "\n",
    "    # Apply the mask to the image\n",
    "    segmented_image = cv2.bitwise_and(image, image, mask=full_mask)\n",
    "\n",
    "    # Save the segmented image\n",
    "    base_name, ext = os.path.splitext(output_base_path)\n",
    "    output_path = f\"{base_name}_segmented{ext}\"\n",
    "    cv2.imwrite(output_path, segmented_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "## Main Pipeline for Processing a Folder of Images\n",
    "\n",
    "The next cell contains the main pipeline function that processes a folder of images. It initializes the Segment Anything Model (SAM), generates segmentations for each image, visualizes and saves the segmentations, and optionally crops and collages the largest masks. The pipeline also includes resource monitoring to track CPU, memory, and GPU usage during processing.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main pipeline processing a folder of images\n",
    "def main_pipeline(input_folder, output_folder):\n",
    "    # Get list of image files in the input folder\n",
    "    image_extensions = ('.jpg', '.jpeg', '.png')\n",
    "    image_files = [f for f in os.listdir(input_folder) if f.lower().endswith(image_extensions)]\n",
    "    num_images = len(image_files)\n",
    "\n",
    "    if num_images == 0:\n",
    "        print(f\"No images found in {input_folder}.\")\n",
    "        return\n",
    "\n",
    "    # Ensure the output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    pbar_lock = threading.Lock()\n",
    "    with tqdm(total=num_images, desc='Processing Images', unit='image') as pbar:\n",
    "        # Start resource monitor thread\n",
    "        stop_event = threading.Event()\n",
    "        monitor_thread = threading.Thread(target=resource_monitor, args=(pbar, stop_event, pbar_lock))\n",
    "        monitor_thread.start()\n",
    "\n",
    "        try:\n",
    "            # Initialize SAM once\n",
    "            with pbar_lock:\n",
    "                pbar.set_description('Initializing SAM')\n",
    "            mask_generator = initialize_sam()\n",
    "\n",
    "            for image_file in image_files:\n",
    "                image_path = os.path.join(input_folder, image_file)\n",
    "                output_path = os.path.join(output_folder, image_file)\n",
    "\n",
    "                with pbar_lock:\n",
    "                    pbar.set_description(f'Processing {image_file}')\n",
    "                    # print(f\"Processing {image_file}\")\n",
    "\n",
    "                try:\n",
    "                    masks, image = generate_segmentation(image_path, mask_generator)\n",
    "                    visualize_and_save_segmentation(image, masks, output_path)\n",
    "                    crop_and_collage_largest_masks(image, masks, output_path)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {image_file}: {e}\")\n",
    "                finally:\n",
    "                    # Clean up to free memory\n",
    "                    torch.cuda.empty_cache()\n",
    "                    gc.collect()\n",
    "\n",
    "                with pbar_lock:\n",
    "                    pbar.update(1)\n",
    "\n",
    "            # Clean up SAM model after processing\n",
    "            del mask_generator\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "        finally:\n",
    "            # Stop the resource monitor thread\n",
    "            stop_event.set()\n",
    "            monitor_thread.join()\n",
    "\n",
    "# Define missing functions\n",
    "def generate_segmentation(image_path, mask_generator):\n",
    "    image = cv2.imread(image_path)\n",
    "    masks = mask_generator.generate(image)\n",
    "    return masks, image\n",
    "\n",
    "def visualize_and_save_segmentation(image, masks, output_path):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image)\n",
    "    for mask in masks:\n",
    "        plt.contour(mask['segmentation'], colors='r')\n",
    "    plt.axis('off')\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "\n",
    "def crop_and_collage_largest_masks(image, masks, output_path):\n",
    "    # Placeholder function for cropping and collaging masks\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Riley\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\segment_anything\\build_sam.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(f)\n"
     ]
    }
   ],
   "source": [
    "# Initialize SAM and the mask generator\n",
    "mask_generator = initialize_sam()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean up to free memory\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda Version: 11.8\n",
      "GPU Used: NVIDIA GeForce RTX 2060 SUPER\n",
      "Current GPU Code Used: 0\n",
      "Number of GPUs installed: 1\n",
      "Starting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 0019_B%2010%200279821.jpg: 100%|██████████| 10/10 [04:05<00:00, 24.51s/image, CPU:19.8%, Mem:52.4%, GPU:9.0%, GPU Mem:66.8%] \n"
     ]
    }
   ],
   "source": [
    "#Optional Run with Resource Monitoring\n",
    "if __name__ == \"__main__\":\n",
    "    # Version checking\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"Cuda Version:\", torch.version.cuda)\n",
    "        print(\"GPU Used:\", torch.cuda.get_device_name(0))\n",
    "        print(\"Current GPU Code Used:\", torch.cuda.current_device())\n",
    "        print(\"Number of GPUs installed:\", torch.cuda.device_count())\n",
    "    else:\n",
    "        print(\"No GPU available\")\n",
    "\n",
    "    print(\"Starting...\")\n",
    "    input_folder = \"c:\\\\Users\\\\Riley\\\\Desktop\\\\TestSet\"  # Update this path as needed\n",
    "    output_folder = \"C:\\\\Users\\\\Riley\\\\Desktop\\\\SEGTESTINGFOLER3\"  # Update this path as needed\n",
    "\n",
    "\n",
    "    #Toggle if you want to test on one image or on a folder of images\n",
    "    # image_path = \"path_to_your_image.jpg\"  # Update this path\n",
    "    # output_path = \"path_to_output_image.jpg\"\n",
    "    main_pipeline(input_folder, output_folder)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
